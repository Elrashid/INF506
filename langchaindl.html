<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.14.4/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@6.7.0"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.14.4"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.14.4/dist/index.umd.min.js"></script><script>(r => {
                setTimeout(r);
              })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{"type":"heading","depth":0,"payload":{"lines":[0,1]},"content":"langchain","children":[{"type":"heading","depth":1,"payload":{"lines":[2,3]},"content":"lesson 01 : models,prompts and parsers","children":[{"type":"heading","depth":2,"payload":{"lines":[4,5]},"content":"Understanding Models, Prompts, and Parsers in NLP","children":[{"type":"heading","depth":3,"payload":{"lines":[5,6]},"content":"Models refer to the language models used in natural language processing (NLP) and can be reusable."},{"type":"heading","depth":3,"payload":{"lines":[6,7]},"content":"Prompts are inputs used in NLP to pass into models for processing."},{"type":"heading","depth":3,"payload":{"lines":[7,8]},"content":"Parsers take the output of models and structure it for downstream tasks."},{"type":"heading","depth":3,"payload":{"lines":[8,9]},"content":"A helper function for NLP processing is provided, similar to the one used in Open AI's GPD Prompt Engineering for Developers course."},{"type":"heading","depth":3,"payload":{"lines":[9,10]},"content":"Language translation can be achieved using an LM by specifying the desired output style as a prompt using an F string."},{"type":"heading","depth":3,"payload":{"lines":[10,11]},"content":"The provided code example can be modified to generate different outputs."}]},{"type":"heading","depth":2,"payload":{"lines":[12,13]},"content":"Using Lang Chain for Language Translation","children":[{"type":"heading","depth":3,"payload":{"lines":[13,14]},"content":"The writer demonstrates a convenient way of using Lang Chain for language translations."},{"type":"heading","depth":3,"payload":{"lines":[14,15]},"content":"The writer imports Chat OpenAI, which is Lang Chain's abstraction for the ChatGPT API endpoint."},{"type":"heading","depth":3,"payload":{"lines":[15,16]},"content":"The temperature parameter is set to 0 to make the output less random."},{"type":"heading","depth":3,"payload":{"lines":[16,17]},"content":"The writer defines a template string to translate text into a specific style."},{"type":"heading","depth":3,"payload":{"lines":[17,18]},"content":"Lang Chain's chat prompt template is imported to create a prompt template using the template string."},{"type":"heading","depth":3,"payload":{"lines":[18,19]},"content":"The writer specifies a style called customer style for the translation."},{"type":"heading","depth":3,"payload":{"lines":[19,20]},"content":"The writer creates customer messages, which will generate prompts to be passed to the ChatGPT API endpoint."},{"type":"heading","depth":3,"payload":{"lines":[20,21]},"content":"The custom message is a list with a prompt as its first element."},{"type":"heading","depth":3,"payload":{"lines":[21,22]},"content":"Chat OpenAI is called to translate the prompt, resulting in a text translated from English pirates to polite American English."}]},{"type":"heading","depth":2,"payload":{"lines":[23,24]},"content":"Using Prompt Templates to Generate Sophisticated Applications","children":[{"type":"heading","depth":3,"payload":{"lines":[24,25]},"content":"Professional writers can use prompt templates to generate sophisticated applications."},{"type":"heading","depth":3,"payload":{"lines":[25,26]},"content":"Prompt templates have a predefined structure that includes a subject title and detailed notes."},{"type":"heading","depth":3,"payload":{"lines":[26,27]},"content":"Prompt templates help in reusing good prompts and in connecting to different APIs without engineering your own prompts."},{"type":"heading","depth":3,"payload":{"lines":[27,28]},"content":"Lang chain's prompt libraries also support output pausing, which helps guide the LM to generate output in a certain format."},{"type":"heading","depth":3,"payload":{"lines":[28,29]},"content":"Writing prompts in a specific format, such as using specific keywords, helps in carrying out complex reasoning frameworks like the React Frameworks and Chain of Thought."}]},{"type":"heading","depth":2,"payload":{"lines":[30,31]},"content":"Using Language Models to Extract Information from Product Reviews","children":[{"type":"heading","depth":3,"payload":{"lines":[31,32]},"content":"Language models (LMs) can provide more accurate conclusions by allowing the LM to think and process information before taking action."},{"type":"heading","depth":3,"payload":{"lines":[32,33]},"content":"Keywords such as thought, action, and observation can be used to prompt LMs to extract relevant information from text."},{"type":"heading","depth":3,"payload":{"lines":[33,34]},"content":"LMs can output JSON that can be parsed by Lang chain to extract desired information from product reviews."},{"type":"heading","depth":3,"payload":{"lines":[34,35]},"content":"An example review is provided, and a review template is used to extract information such as whether the product was a gift, delivery time, and price value."},{"type":"heading","depth":3,"payload":{"lines":[35,36]},"content":"The extracted information can then be formatted in JSON with specific keys and values."},{"type":"heading","depth":3,"payload":{"lines":[36,37]},"content":"Lang chain can be used to wrap the prompt template and extract information from customer reviews in a streamlined manner."}]},{"type":"heading","depth":2,"payload":{"lines":[38,39]},"content":"Using Line Chain Parser for JSON Response","children":[{"type":"heading","depth":3,"payload":{"lines":[39,40]},"content":"The tutorial guides on using Line Chain Parser for parsing the JSON response."},{"type":"heading","depth":3,"payload":{"lines":[40,41]},"content":"The author explains how to extract specific values from the JSON response."},{"type":"heading","depth":3,"payload":{"lines":[41,42]},"content":"The response content is a string but appears like a dictionary with key value pairs."},{"type":"heading","depth":3,"payload":{"lines":[42,43]},"content":"Line Chain Parser can extract schema from the response content to determine the structure of the response."},{"type":"heading","depth":3,"payload":{"lines":[43,44]},"content":"The author imports response schema and structured output parser from Line Chain."},{"type":"heading","depth":3,"payload":{"lines":[44,45]},"content":"The obtained schema is then put into a list."},{"type":"heading","depth":3,"payload":{"lines":[45,46]},"content":"The author formats the instructions to give to the language model by using a review template."},{"type":"heading","depth":3,"payload":{"lines":[46,47]},"content":"The review template includes the schema and the format instructions generated by Line Chain."},{"type":"heading","depth":3,"payload":{"lines":[47,48]},"content":"The author calls the OpenAI endpoint and uses the output parser to extract an output dictionary from the JSON response."},{"type":"heading","depth":3,"payload":{"lines":[48,49]},"content":"The format output dictionary allows the author to extract the value associated with a specific key."}]},{"type":"heading","depth":2,"payload":{"lines":[50,51]},"content":"Python Dictionary and LM Output Parsing","children":[{"type":"heading","depth":3,"payload":{"lines":[51,52]},"content":"The text offers a way to parse Language Model output into a Python dictionary for easy downstream processing."},{"type":"heading","depth":3,"payload":{"lines":[52,53]},"content":"The code should be run by the viewer to understand the output and process."},{"type":"heading","depth":3,"payload":{"lines":[53,54]},"content":"The tools of models, prompts, and parsers can be used together to reuse prompt templates and share them with others."},{"type":"heading","depth":3,"payload":{"lines":[54,55]},"content":"Output parsers can help to input prompts and translate them into a specific format, which can then be stored in a Python dictionary for efficient downstream processing."},{"type":"heading","depth":3,"payload":{"lines":[55,56]},"content":"The video promotes Lang Cane as a tool for building effective chatbots that handle past conversation records."}]}]},{"type":"heading","depth":1,"payload":{"lines":[57,58]},"content":"lesson 02 : memory","children":[{"type":"heading","depth":2,"payload":{"lines":[59,60]},"content":"Memory Management in Chatbot Conversations","children":[{"type":"heading","depth":3,"payload":{"lines":[60,61]},"content":"Natural interaction with language models lacks the ability to remember previous conversations or interactions, which is problematic for chatbot applications."},{"type":"heading","depth":3,"payload":{"lines":[61,62]},"content":"Memory management is a way to store previous parts of a conversation and feed it back into the language model for a more conversational flow."},{"type":"heading","depth":3,"payload":{"lines":[62,63]},"content":"An opener API key and several tools are needed to manage chatbot conversations in this way."},{"type":"heading","depth":3,"payload":{"lines":[63,64]},"content":"Land chain is a helpful tool in memory management that allows users to build conversation chains within the chat interface."},{"type":"heading","depth":3,"payload":{"lines":[64,65]},"content":"The conversation memory is stored as a buffer and can be accessed later in the conversation."},{"type":"heading","depth":3,"payload":{"lines":[65,66]},"content":"The verbose variable can be changed to show the system’s prompts and conversation generation."},{"type":"heading","depth":3,"payload":{"lines":[66,67]},"content":"The memory, or history, of the conversation gets longer as more interactions occur."}]},{"type":"heading","depth":2,"payload":{"lines":[68,69]},"content":"Understanding Line Chain Conversation Memory","children":[{"type":"heading","depth":3,"payload":{"lines":[69,70]},"content":"The memory.buffer stores the conversation history, and it can be printed by running the &quot;print memory.buffer&quot; command."},{"type":"heading","depth":3,"payload":{"lines":[70,71]},"content":"Memory.load command reads the variables stored in the memory and returns an empty dictionary enclosed in curly braces."},{"type":"heading","depth":3,"payload":{"lines":[71,72]},"content":"Line Chain uses the conversation buffer memory to store the conversation so far."},{"type":"heading","depth":3,"payload":{"lines":[72,73]},"content":"You can add new inputs and outputs to the memory explicitly using &quot;memory.save context&quot; command."},{"type":"heading","depth":3,"payload":{"lines":[73,74]},"content":"Additional contacts can be kept by saving them to memory using the memory.save command."},{"type":"heading","depth":3,"payload":{"lines":[74,75]},"content":"The memory.status shows the current state of the memory, including the saved inputs and outputs."},{"type":"heading","depth":3,"payload":{"lines":[75,76]},"content":"You can print out the memory to see the new data stored in it."}]},{"type":"heading","depth":2,"payload":{"lines":[77,78]},"content":"The Role of Memory in Chatbots","children":[{"type":"heading","depth":3,"payload":{"lines":[78,79]},"content":"Large language models used in chatbots are stateless and do not retain memory of previous conversations."},{"type":"heading","depth":3,"payload":{"lines":[79,80]},"content":"The chatbot relies on a rapid code to provide the full conversation context as memory for each API endpoint transaction."},{"type":"heading","depth":3,"payload":{"lines":[80,81]},"content":"As conversations become longer, the amount of memory required increases and can lead to higher token charges."},{"type":"heading","depth":3,"payload":{"lines":[81,82]},"content":"Line Chain offers various memory options, including a conversation buffer window memory that only retains a window of memory."},{"type":"heading","depth":3,"payload":{"lines":[82,83]},"content":"Setting k to 1 in the conversational buffer window memory remembers one recent conversational exchange, preventing memory from growing infinitely."},{"type":"heading","depth":3,"payload":{"lines":[83,84]},"content":"By using conversational token buffer memory, the cost of LM calls is based more directly on the number of tokens."}]},{"type":"heading","depth":2,"payload":{"lines":[85,86]},"content":"Different ways to handle conversation memory in AI chatbots","children":[{"type":"heading","depth":3,"payload":{"lines":[86,87]},"content":"Increasing token limit in AI chatbot conversation can expand the memory capacity to store more conversation history"},{"type":"heading","depth":3,"payload":{"lines":[87,88]},"content":"Decreasing token limit in AI chatbot conversation results in a limited memory that retains only the most recent exchanges"},{"type":"heading","depth":3,"payload":{"lines":[88,89]},"content":"Memory can be set using an OOM to count tokens in a specific way depending on the LMS used"},{"type":"heading","depth":3,"payload":{"lines":[89,90]},"content":"Implementing conversation summary buffer memory in AI chatbots stores a summary of the conversation so far, and the memory capacity limit can be changed with an OOM"},{"type":"heading","depth":3,"payload":{"lines":[90,91]},"content":"The conversational summary is created by the OOM using the entire conversation history up to the token limit, which can be generated by the OpenAI endpoint."}]},{"type":"heading","depth":2,"payload":{"lines":[92,93]},"content":"Constitution AI Development","children":[{"type":"heading","depth":3,"payload":{"lines":[93,94]},"content":"The writer suggests showcasing the latest NRP (Natural Language Processing) capabilities in Constitution AI development."},{"type":"heading","depth":3,"payload":{"lines":[94,95]},"content":"The writer considers an open source framework to build cool NLP applications using OMS (Ontology Management System)."},{"type":"heading","depth":3,"payload":{"lines":[95,96]},"content":"The AI system output incorporates the most recent conversation summary buffer memory."},{"type":"heading","depth":3,"payload":{"lines":[96,97]},"content":"The explicit storage of messages is capped at 100 tokens and anything beyond that uses CLM (Continual Learning Model) to generate a summary."},{"type":"heading","depth":3,"payload":{"lines":[97,98]},"content":"The writer explains that although the example uses a chat, these memories are useful for other applications that receive new snippets of text or information repeatedly."}]},{"type":"heading","depth":2,"payload":{"lines":[99,100]},"content":"Memory Types in Lang Chain","children":[{"type":"heading","depth":3,"payload":{"lines":[100,101]},"content":"The video explains the need to limit the memory used to store the drawing list of facts."},{"type":"heading","depth":3,"payload":{"lines":[101,102]},"content":"The available memory types in Lang Chain include buffer memories, token memory, and summary memory."},{"type":"heading","depth":3,"payload":{"lines":[102,103]},"content":"Lang Chain also incorporates vector data memory to store text embeddings for retrieving relevant blocks of text."},{"type":"heading","depth":3,"payload":{"lines":[103,104]},"content":"Entity memory is applicable when remembering details about specific people or entities."},{"type":"heading","depth":3,"payload":{"lines":[104,105]},"content":"Lang Chain supports the idea of using multiple memory types, such as conversation and entity memory, to recall important information."},{"type":"heading","depth":3,"payload":{"lines":[105,106]},"content":"Developers can also store the entire conversation in a conventional database for auditing or system improvement."}]}]},{"type":"heading","depth":1,"payload":{"lines":[109,110]},"content":"lesson 03 : Chains","children":[{"type":"heading","depth":2,"payload":{"lines":[111,112]},"content":"Building Blocks of LOM Chains","children":[{"type":"heading","depth":3,"payload":{"lines":[112,113]},"content":"Harrison introduces LOM chains as the key building block of land chains."},{"type":"heading","depth":3,"payload":{"lines":[113,114]},"content":"The chain employs an LM large language model and a prompt."},{"type":"heading","depth":3,"payload":{"lines":[114,115]},"content":"Multiple building blocks can be combined to execute operations on text or data."},{"type":"heading","depth":3,"payload":{"lines":[115,116]},"content":"First, environment variables and data are loaded, including a pandas data frame with product and review columns."},{"type":"heading","depth":3,"payload":{"lines":[116,117]},"content":"The LOM chain is simple but powerful and requires OpenAI model, a chat prompt template, and the L Alum chain to be imported."},{"type":"heading","depth":3,"payload":{"lines":[117,118]},"content":"A language model is initialized with a high temperature."},{"type":"heading","depth":3,"payload":{"lines":[118,119]},"content":"A prompt is initialized to input a product and generate the best name for the company."},{"type":"heading","depth":3,"payload":{"lines":[119,120]},"content":"The LLN chain is created by combining the language model and the prompt."},{"type":"heading","depth":3,"payload":{"lines":[120,121]},"content":"Example of running the chain is demonstrated by using 'chain.dot.run' and passing the prompt into LOM."}]},{"type":"heading","depth":2,"payload":{"lines":[123,124]},"content":"How to use Sequential Chains in Custom Writing","children":[{"type":"heading","depth":3,"payload":{"lines":[124,125]},"content":"The LLM chain is the most basic type of chain that is used frequently in custom writing."},{"type":"heading","depth":3,"payload":{"lines":[125,126]},"content":"Sequential chains run a sequence of chains one after another, making it easier to run sub-chains that expect only one input and return only one output."},{"type":"heading","depth":3,"payload":{"lines":[126,127]},"content":"The first chain can use LLM and a prompt to take in the product and return the best name to describe that company."},{"type":"heading","depth":3,"payload":{"lines":[127,128]},"content":"In the second chain, the company name is taken in as input and then a 20-word description is output."},{"type":"heading","depth":3,"payload":{"lines":[128,129]},"content":"Simple sequential chains work well when there's only a single input and a single output."},{"type":"heading","depth":3,"payload":{"lines":[129,130]},"content":"Regular sequential chains are used when there are multiple inputs or outputs."},{"type":"heading","depth":3,"payload":{"lines":[130,131]},"content":"The first chain translates the review into English, the second chain creates a summary in one sentence, and the third chain detects the language of the original review."}]},{"type":"heading","depth":2,"payload":{"lines":[132,133]},"content":"Explanation of Chaining in Natural Language Processing (NLP)","children":[{"type":"heading","depth":3,"payload":{"lines":[133,134]},"content":"The first chain is used to take in a review variable from the user."},{"type":"heading","depth":3,"payload":{"lines":[134,135]},"content":"The second chain takes in the first chain’s output (review) and calculates the summary in English language, and outputs it."},{"type":"heading","depth":3,"payload":{"lines":[135,136]},"content":"The third chain takes in the review and detects the language, and outputs the language variable."},{"type":"heading","depth":3,"payload":{"lines":[136,137]},"content":"The fourth chain takes in both the outputs of the second and third chains, and generates a follow-up response in the specified language."},{"type":"heading","depth":3,"payload":{"lines":[137,138]},"content":"All sub-chains have precise input and output keys. The simple sequential chain takes input from the previous chain and outputs intermediate values like English review, summary, and follow-up message."},{"type":"heading","depth":3,"payload":{"lines":[138,139]},"content":"The operation can be tweaked according to the input variables routed to the appropriate chains."}]},{"type":"heading","depth":2,"payload":{"lines":[140,141]},"content":"Designing a Routing System for Multiple Sub Chains","children":[{"type":"heading","depth":3,"payload":{"lines":[141,142]},"content":"The routing system should have a single router chain that decides which sub chain to pass the input to based on its subject."},{"type":"heading","depth":3,"payload":{"lines":[142,143]},"content":"Different types of prompt templates should be defined for the sub chains, each with a unique name and description specifying the subject they are best suited for."},{"type":"heading","depth":3,"payload":{"lines":[143,144]},"content":"A multi-prompt chain should be imported to handle routing between multiple prompt templates."},{"type":"heading","depth":3,"payload":{"lines":[144,145]},"content":"An LLM router chain, which uses a language model to route between different sub chains, should also be implemented."},{"type":"heading","depth":3,"payload":{"lines":[145,146]},"content":"A router output parser should be used to parse the language model output into a dictionary that determines which chain to use and what input to provide to that chain."},{"type":"heading","depth":3,"payload":{"lines":[146,147]},"content":"Destination chains should be defined, consisting of language model chains for each sub chain and a default chain that is called when no suitable sub chain is found."},{"type":"heading","depth":3,"payload":{"lines":[147,148]},"content":"A flexible router template should be created that can be formatted with different types of destinations to handle various subjects."}]},{"type":"heading","depth":2,"payload":{"lines":[149,150]},"content":"Creating a Router Chain for Different Types of Input","children":[{"type":"heading","depth":3,"payload":{"lines":[150,151]},"content":"A router chain can specialize in a particular type of input and then pass it to the corresponding sub chain."},{"type":"heading","depth":3,"payload":{"lines":[151,152]},"content":"The router chain can decide which sub chain to use based on the subject of the input prompt."},{"type":"heading","depth":3,"payload":{"lines":[152,153]},"content":"The different prompts can be defined for various subjects, such as physics, math, history, and computer science, and given a name and description."},{"type":"heading","depth":3,"payload":{"lines":[153,154]},"content":"The concept of multi-prompt chains allows for routing between different prompt templates."},{"type":"heading","depth":3,"payload":{"lines":[154,155]},"content":"The LLM router chain uses language models to route between sub chains based on their name and description."},{"type":"heading","depth":3,"payload":{"lines":[155,156]},"content":"The router output parser is used to parse the output into a dictionary that can be used downstream by the different chains."},{"type":"heading","depth":3,"payload":{"lines":[156,157]},"content":"Creating a full router template by formatting it with the destination chains creates a flexible template for different types of subjects."},{"type":"heading","depth":3,"payload":{"lines":[157,158]},"content":"The overall chain includes the router chain, destination chains, and a default chain for unrelated prompts."},{"type":"heading","depth":3,"payload":{"lines":[158,159]},"content":"The router chain is used to route inputs to the corresponding sub-chains, such as the physics, math, history, or computer science chains."},{"type":"heading","depth":3,"payload":{"lines":[159,160]},"content":"The default chain is used when the input prompt has nothing to do with any of the sub-chains."}]}]},{"type":"heading","depth":1,"payload":{"lines":[165,166]},"content":"lesson 04 : Question and Answer","children":[{"type":"heading","depth":2,"payload":{"lines":[170,171]},"content":"Building Applications Using Language Models for Document Retrieval","children":[{"type":"heading","depth":3,"payload":{"lines":[171,172]},"content":"One of the most common complex applications using LLMs is a system that can answer questions based on a document."},{"type":"heading","depth":3,"payload":{"lines":[172,173]},"content":"The document can be extracted from a PDF, a webpage, or from a company's internal document collection."},{"type":"heading","depth":3,"payload":{"lines":[173,174]},"content":"This system helps users gain deeper understanding and access to the information they need."},{"type":"heading","depth":3,"payload":{"lines":[174,175]},"content":"LLMs combined with data they weren't trained on makes them flexible and adaptable to various situations."},{"type":"heading","depth":3,"payload":{"lines":[175,176]},"content":"Embeddings and vector stores are some of the most powerful modern techniques for building these systems."},{"type":"heading","depth":3,"payload":{"lines":[176,177]},"content":"Retrieval QA chain, Chat Open AI language model, and CSV loader are some of the tools needed when building this chain."},{"type":"heading","depth":3,"payload":{"lines":[177,178]},"content":"Docker A in-memory search Vector store is an in-memory vector store used to get started with the vector store."},{"type":"heading","depth":3,"payload":{"lines":[178,179]},"content":"The vector store is used for indexing documents for faster retrieval."}]},{"type":"heading","depth":2,"payload":{"lines":[180,181]},"content":"How to Use Language Models to Answer Questions over Your Documents","children":[{"type":"heading","depth":3,"payload":{"lines":[181,182]},"content":"The text describes how easy it is to get started with using language models for document question answering without an external database."},{"type":"heading","depth":3,"payload":{"lines":[182,183]},"content":"To use the provided CSV of Outdoor Clothing, the text initializes a CSV loader and a vector store index creator."},{"type":"heading","depth":3,"payload":{"lines":[183,184]},"content":"The vector store index creator helps create a vector store with just a few lines of code."},{"type":"heading","depth":3,"payload":{"lines":[184,185]},"content":"Embeddings create numerical representations for text, capturing the semantic meaning."},{"type":"heading","depth":3,"payload":{"lines":[185,186]},"content":"Pieces of text with similar content will have similar vectors, allowing for comparison in the vector space."}]},{"type":"heading","depth":2,"payload":{"lines":[187,188]},"content":"Using Vector Database for Question Answering","children":[{"type":"heading","depth":3,"payload":{"lines":[188,189]},"content":"The article discusses using a vector database to store vector representations of chunks of text."},{"type":"heading","depth":3,"payload":{"lines":[189,190]},"content":"The vector database is populated with text chunks from incoming documents, which are first broken down into smaller pieces."},{"type":"heading","depth":3,"payload":{"lines":[190,191]},"content":"Each chunk is then embedded using OpenAI's embedding class and stored in the vector database."},{"type":"heading","depth":3,"payload":{"lines":[191,192]},"content":"During runtime, an incoming query is also embedded and compared to all vectors in the database to select the N most similar chunks."},{"type":"heading","depth":3,"payload":{"lines":[192,193]},"content":"These chunks are then passed to a language model to obtain a final answer."},{"type":"heading","depth":3,"payload":{"lines":[193,194]},"content":"The vector database is created using the 'from documents' method on the vector store, which takes in a list of documents and an embedding object."},{"type":"heading","depth":3,"payload":{"lines":[194,195]},"content":"The use of vector database allows finding relevant text pieces efficiently for answer generation."}]},{"type":"heading","depth":2,"payload":{"lines":[196,197]},"content":"Creating a Retrieval-QA Chain for Question Answering","children":[{"type":"heading","depth":3,"payload":{"lines":[197,198]},"content":"The Vector store can be used to create a retriever, which is a generic interface that returns documents for a given query."},{"type":"heading","depth":3,"payload":{"lines":[198,199]},"content":"Retriever can be underpinned by any method that takes in a query and returns documents, such as a vector store or embeddings."},{"type":"heading","depth":3,"payload":{"lines":[199,200]},"content":"By importing a language model like Chat Open AI, we can generate a natural language response."},{"type":"heading","depth":3,"payload":{"lines":[200,201]},"content":"To generate an accurate response, the documents need to be combined into a single piece of text."},{"type":"heading","depth":3,"payload":{"lines":[201,202]},"content":"This can be done with the lane chain, which creates a retriever QA chain that retrieves documents and then performs question answering using the language model."},{"type":"heading","depth":3,"payload":{"lines":[202,203]},"content":"The simplest method for the chain is Stuff, which stuffs all the documents into context and makes one call to the language model."}]},{"type":"heading","depth":2,"payload":{"lines":[204,205]},"content":"How to use Link Chain for question answering","children":[{"type":"heading","depth":3,"payload":{"lines":[205,206]},"content":"To use a retriever, which is an interface for fetching documents, in Link Chain, pass in the retriever and set verbose to true to fetch and pass the documents to the language model."},{"type":"heading","depth":3,"payload":{"lines":[206,207]},"content":"Create a query and run the chain on it to get the response, which can be displayed using display and markdown utilities."},{"type":"heading","depth":3,"payload":{"lines":[207,208]},"content":"Link Chain can be used in one line or with more detailed steps to set specific parameters for customization."},{"type":"heading","depth":3,"payload":{"lines":[208,209]},"content":"The index can also be customized with different embeddings and vector stores."},{"type":"heading","depth":3,"payload":{"lines":[209,210]},"content":"The &quot;stuff&quot; method is a simple way to send a prompt to the language model and get back a response."},{"type":"heading","depth":3,"payload":{"lines":[210,211]},"content":"Mapreduce is a method that takes all the chunks of documents, passes them along with the query to a language model, and summarizes the responses into a final answer."},{"type":"heading","depth":3,"payload":{"lines":[211,212]},"content":"Refine is another method that iteratively builds upon the answer from previous documents."},{"type":"heading","depth":3,"payload":{"lines":[212,213]},"content":"Math-free rank is an experimental method where a single call is made to the language model for each document, with a score returned and the highest score selected."}]},{"type":"heading","depth":2,"payload":{"lines":[214,215]},"content":"Natural Language Processing Methods for Document Processing","children":[{"type":"heading","depth":3,"payload":{"lines":[215,216]},"content":"Language model calls can lead to high costs and require specific instructions for relevant documents."},{"type":"heading","depth":3,"payload":{"lines":[216,217]},"content":"Batch processing of independent calls using mapreduce and stuff methods can be relatively fast."},{"type":"heading","depth":3,"payload":{"lines":[217,218]},"content":"The stuff method combines chunks of documents into one, while the mapreduce method sends them to the language model."},{"type":"heading","depth":3,"payload":{"lines":[218,219]},"content":"These methods can be used for various document processing tasks, including question answering and summarization."},{"type":"heading","depth":3,"payload":{"lines":[219,220]},"content":"Mapreduce is commonly used for recursively summarizing large documents."}]}]},{"type":"heading","depth":1,"payload":{"lines":[226,227]},"content":"lesson 05 : Evaluation","children":[{"type":"heading","depth":2,"payload":{"lines":[229,230]},"content":"Evaluating LM-based Applications","children":[{"type":"heading","depth":3,"payload":{"lines":[230,231]},"content":"One of the tricky steps when building a complex application using an ORM is evaluating how well the application is doing, and if it meets the accuracy criteria."},{"type":"heading","depth":3,"payload":{"lines":[231,232]},"content":"If you decide to change your implementation, it can be challenging to determine if you are making it better or worse."},{"type":"heading","depth":3,"payload":{"lines":[232,233]},"content":"Harrison discusses how to evaluate an LM-based application, including how to use visualizers and debuggers to understand each step of the model."},{"type":"heading","depth":3,"payload":{"lines":[233,234]},"content":"Evaluating language models themselves can also be useful for evaluating other language models and applications."},{"type":"heading","depth":3,"payload":{"lines":[234,235]},"content":"With development shifting to prompt-based development and the use of LMs, the evaluation process is being rethought."},{"type":"heading","depth":3,"payload":{"lines":[235,236]},"content":"To start evaluating an application, you need to have the chain or application and create a retrieval QA chain by specifying the language model, chain type, retriever, and verbosity."},{"type":"heading","depth":3,"payload":{"lines":[236,237]},"content":"The first step in evaluating an application is to identify the data points that you want to evaluate it on."}]},{"type":"heading","depth":2,"payload":{"lines":[238,239]},"content":"Methods for Generating Question-Answer Pairs from Data Points","children":[{"type":"heading","depth":3,"payload":{"lines":[239,240]},"content":"One method for generating question-answer pairs involves manually identifying data points from a set of documents, and creating corresponding example queries and ground truth answers."},{"type":"heading","depth":3,"payload":{"lines":[240,241]},"content":"This approach can be time-consuming and may not scale well."},{"type":"heading","depth":3,"payload":{"lines":[241,242]},"content":"An alternative method uses a language model, such as the QA Generation chain, to automatically generate question-answer pairs from documents."},{"type":"heading","depth":3,"payload":{"lines":[242,243]},"content":"The apply and parse method can be used to output a dictionary with query-answer pairs."},{"type":"heading","depth":3,"payload":{"lines":[243,244]},"content":"This method saves time and can be used for more complicated tasks."},{"type":"heading","depth":3,"payload":{"lines":[244,245]},"content":"Generated question-answer pairs can be added to the examples for later evaluation."}]},{"type":"heading","depth":2,"payload":{"lines":[246,247]},"content":"Debugging Language Chains in OpenAI","children":[{"type":"heading","depth":3,"payload":{"lines":[247,248]},"content":"To evaluate the language chain in OpenAI, run an example through the chain and analyze the output it produces."},{"type":"heading","depth":3,"payload":{"lines":[248,249]},"content":"Simply getting back an answer is a little limiting in terms of what we can see inside the chain, such as the actual prompt going into the language model or the retrieved documents."},{"type":"heading","depth":3,"payload":{"lines":[249,250]},"content":"To get a more comprehensive view of the chain, use the Lane chain debug utility in OpenAI by setting Lane chain debug equals true and rerunning the example."},{"type":"heading","depth":3,"payload":{"lines":[250,251]},"content":"The debug utility provides information about each step of the chain, such as retrieval QA chain and the stuffed documents chain, and allows you to closely examine the inputs and outputs of each step."},{"type":"heading","depth":3,"payload":{"lines":[251,252]},"content":"This process can help identify issues with the retrieval step that may affect the result of the language model and allow you to track tokens and costs for each step in the chain."}]},{"type":"heading","depth":2,"payload":{"lines":[253,254]},"content":"Debugging and Evaluating a Language Model with Lane Chain","children":[{"type":"heading","depth":3,"payload":{"lines":[254,255]},"content":"To evaluate a language model, it's helpful to run an example through the chain and examine the output produced."},{"type":"heading","depth":3,"payload":{"lines":[255,256]},"content":"However, this can be limiting; it's important to understand what's happening inside the chain and what prompts and documents are being used."},{"type":"heading","depth":3,"payload":{"lines":[256,257]},"content":"Lane Chain debug can assist with this - setting it to true yields more information."},{"type":"heading","depth":3,"payload":{"lines":[257,258]},"content":"It's often not the language model but the retrieval step that causes incorrect results, so examining the question and context can help debug issues."},{"type":"heading","depth":3,"payload":{"lines":[258,259]},"content":"It's possible to evaluate multiple examples manually, but this can be tedious."},{"type":"heading","depth":3,"payload":{"lines":[259,260]},"content":"A language model can be used to evaluate examples."},{"type":"heading","depth":3,"payload":{"lines":[260,261]},"content":"The QA question answering eval chain can help with evaluation."},{"type":"heading","depth":3,"payload":{"lines":[261,262]},"content":"Examples, predictions, and graded outputs are used to assess performance."},{"type":"heading","depth":3,"payload":{"lines":[262,263]},"content":"To evaluate an example, question, real answer, predicted answer, and grade are printed out."}]},{"type":"heading","depth":2,"payload":{"lines":[264,265]},"content":"Importance of Language Models in Text Evaluation","children":[{"type":"heading","depth":3,"payload":{"lines":[265,266]},"content":"The prediction of correct answers using language models is often more accurate than exact or regex matching."},{"type":"heading","depth":3,"payload":{"lines":[266,267]},"content":"Language models help to grade semantically similar answers that vary in wording and structure."},{"type":"heading","depth":3,"payload":{"lines":[267,268]},"content":"The evaluation of language models for open-ended text tasks is difficult and requires developing new heuristics and metrics."},{"type":"heading","depth":3,"payload":{"lines":[268,269]},"content":"Using a link-chain evaluation platform can help track inputs and outputs of text evaluations."},{"type":"heading","depth":3,"payload":{"lines":[269,270]},"content":"These evaluations are important for developing and improving language models for natural language generation."}]},{"type":"heading","depth":2,"payload":{"lines":[271,272]},"content":"Adding Examples to a Data Set for Evaluation","children":[{"type":"heading","depth":3,"payload":{"lines":[272,273]},"content":"The system message, human question, chat model response, and output metadata are displayed for review."},{"type":"heading","depth":3,"payload":{"lines":[273,274]},"content":"Examples can be added to a data set by clicking a button and inputting the query and output results."},{"type":"heading","depth":3,"payload":{"lines":[274,275]},"content":"The data set can be given a name, such as 'deep learning', and examples can be continuously added for evaluation."},{"type":"heading","depth":3,"payload":{"lines":[275,276]},"content":"Evaluation is necessary and building up a database of examples is a good way to have it running in the background."}]}]},{"type":"heading","depth":1,"payload":{"lines":[278,279]},"content":"Lesson 06 : Agents","children":[{"type":"heading","depth":2,"payload":{"lines":[280,281]},"content":"Using Large Language Models as Reasoning Engines with Lang Chain Agents","children":[{"type":"heading","depth":3,"payload":{"lines":[281,282]},"content":"Large language models can be thought of as a knowledge store, with the ability to memorize vast amounts of information from the internet."},{"type":"heading","depth":3,"payload":{"lines":[282,283]},"content":"An even more useful way to view a large language model is as a reasoning engine that can process chunks of text or other information sources to help answer questions, reason through content, or determine next steps."},{"type":"heading","depth":3,"payload":{"lines":[283,284]},"content":"The Lang Chain Agents Framework enables the creation and utilization of agents, which are powerful tools that can interact with various data stores, APIs, and functions."},{"type":"heading","depth":3,"payload":{"lines":[284,285]},"content":"Setting environment variables and importing necessary components precedes initializing a language model."},{"type":"heading","depth":3,"payload":{"lines":[285,286]},"content":"When using a language model as the reasoning engine for an agent, it is important to set the temperature to zero for maximum precision and effectiveness."}]},{"type":"heading","depth":2,"payload":{"lines":[287,288]},"content":"Illustration of a Conversational AI Coding Process","children":[{"type":"heading","depth":3,"payload":{"lines":[288,289]},"content":"The author sets a numerical value to 0 to eliminate any randomness."},{"type":"heading","depth":3,"payload":{"lines":[289,290]},"content":"The author loads two tools: LLM Math Tool and Wikipedia Tool."},{"type":"heading","depth":3,"payload":{"lines":[290,291]},"content":"The LLM Math tool utilizes a language model and calculator for solving math problems. The Wikipedia Tool queries Wikipedia and retrieves search results."},{"type":"heading","depth":3,"payload":{"lines":[291,292]},"content":"The author initializes an agent with the tools, language model, agent type, and parsing error handling."},{"type":"heading","depth":3,"payload":{"lines":[292,293]},"content":"The agent is optimized for chat models and prompted with a technique called &quot;React&quot; to increase reasoning performance."},{"type":"heading","depth":3,"payload":{"lines":[293,294]},"content":"The author passes verbose and debug equals true, displaying specific details during the coding process."},{"type":"heading","depth":3,"payload":{"lines":[294,295]},"content":"The agent receives a math problem and correctly identifies the tool to use (calculator). It then provides an action input that solves the problem (25% of 300 is 75)."},{"type":"heading","depth":3,"payload":{"lines":[295,296]},"content":"The agent receives a question about Tom Mitchell and selects the Wikipedia tool to retrieve search results."},{"type":"heading","depth":3,"payload":{"lines":[296,297]},"content":"The agent receives two pages of results—one for a computer scientist with the same name and another for an Australian footballer."},{"type":"heading","depth":3,"payload":{"lines":[297,298]},"content":"The agent processes the information from the computer scientist page and presents the name of his book, &quot;Machine Learning,&quot; as the answer to the question given."}]},{"type":"heading","depth":2,"payload":{"lines":[299,300]},"content":"Using AI to Perform Math, Search Queries, and Writing Code","children":[{"type":"heading","depth":3,"payload":{"lines":[300,301]},"content":"The llm math tool is used to perform math calculations using a language model and calculator, while the Wikipedia tool allows the user to run queries against Wikipedia for search results."},{"type":"heading","depth":3,"payload":{"lines":[301,302]},"content":"An agent is initialized using the language model and chat zero shot react description, optimized for chat models, prompting techniques, and parsing errors."},{"type":"heading","depth":3,"payload":{"lines":[302,303]},"content":"The agent executes a math question to calculate 25% of 300 and uses Wikipedia to search for Tom Mitchell, returning a summary of the computer scientist's page and answers the question 'What book did Tom Mitchell write?'"},{"type":"heading","depth":3,"payload":{"lines":[303,304]},"content":"An example of the agent writing code is demonstrated by sorting a list of names by last and first name using the python reple tool, printing the output to be fed back into the language model for reasoning."}]},{"type":"heading","depth":2,"payload":{"lines":[305,306]},"content":"Understanding the action and action input format for an agent","children":[{"type":"heading","depth":3,"payload":{"lines":[306,307]},"content":"The format for the action and action input is different than before, using a Python REPL agent type and code input."},{"type":"heading","depth":3,"payload":{"lines":[307,308]},"content":"The agent creates a list of customer names, sorts them, and prints them out."},{"type":"heading","depth":3,"payload":{"lines":[308,309]},"content":"The agent executor is the top-level runner, followed by the LLM chain for prompt and LLMs."},{"type":"heading","depth":3,"payload":{"lines":[309,310]},"content":"The language model generates the formatted prompt and includes tool access and output formatting instructions."},{"type":"heading","depth":3,"payload":{"lines":[310,311]},"content":"The tool used is the Python REPL, with code input to create and sort a list of customer names."},{"type":"heading","depth":3,"payload":{"lines":[311,312]},"content":"The output of the tool is the printed list of customer names."},{"type":"heading","depth":3,"payload":{"lines":[312,313]},"content":"The agent scratchpad combines previous generation and tool output for reasoning in language model input."}]},{"type":"heading","depth":2,"payload":{"lines":[314,315]},"content":"Explanation of how to customize tools in Agents","children":[{"type":"heading","depth":3,"payload":{"lines":[315,316]},"content":"The text describes the print statements that are used to follow the sequence in which the language model exhausts all its tasks."},{"type":"heading","depth":3,"payload":{"lines":[316,317]},"content":"It explains how the debug mode can be used to troubleshoot any issues that an agent may have while performing its tasks."},{"type":"heading","depth":3,"payload":{"lines":[317,318]},"content":"The text also highlights the fact that agents can be connected to various APIs, data sources, and custom tools to perform their designated tasks."},{"type":"heading","depth":3,"payload":{"lines":[318,319]},"content":"The import tool decorator is introduced as a means of creating a custom tool that can be used by agents."},{"type":"heading","depth":3,"payload":{"lines":[319,320]},"content":"The docstring is explained as a means of communicating to the agent when and how to use the custom tool."},{"type":"heading","depth":3,"payload":{"lines":[320,321]},"content":"Finally, the text creates an agent that utilizes the custom tool to return today's date in response to a query."}]},{"type":"heading","depth":2,"payload":{"lines":[322,323]},"content":"An Overview of Language Models","children":[{"type":"heading","depth":3,"payload":{"lines":[323,324]},"content":"Language models are a type of artificial intelligence that uses algorithms to generate text based on patterns from training data."},{"type":"heading","depth":3,"payload":{"lines":[324,325]},"content":"They can be used for a range of tasks such as chatbots, speech recognition, or machine translation."},{"type":"heading","depth":3,"payload":{"lines":[325,326]},"content":"Language models work by analyzing large amounts of text data and learning patterns to predict what comes next."},{"type":"heading","depth":3,"payload":{"lines":[326,327]},"content":"OpenAI's GPT-3 is a notable language model that has made headlines for its ability to perform a range of tasks with or without human supervision."},{"type":"heading","depth":3,"payload":{"lines":[327,328]},"content":"Despite their capabilities, language models have also been criticized for their potential to generate fake news or misinformation."}]}]}]},{"colorFreezeLevel":2,"maxWidth":300,"initialExpandLevel":1})</script>
</body>
</html>
